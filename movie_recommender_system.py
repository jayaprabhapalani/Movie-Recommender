# -*- coding: utf-8 -*-
"""Movie_recommender_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zYEQL-gKTp_Urba2ircioRJmdaskENMj
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

movies=pd.read_csv('/content/drive/MyDrive/tmdb_5000_movies.csv')
credits=pd.read_csv('/content/drive/MyDrive/tmdb_5000_credits.csv')

movies.head()

credits.head()

credits.info()

movies.info()

movies= movies.merge(credits,on='title')

movies.shape

movies.info()

"""budget,homepage is not a useful column
production company and countries,releasedate,renenue,runtime,spoken_lang,status, popularity ---
"""

movies['original_language'].value_counts()

"""here 90% movies have english as their og lang--- if we consider this for our model -- it will lead to imbalanced recommendation"""

#important column
#genres
#id
#keywords
#title
#overview
#cast
#crew

movies=movies[['movie_id','title','overview','genres','keywords','cast','crew']]

movies.head()

#lets merge the #genres,#keywords,#overview,#cast,#crew and create a column as tag
movies

"""lets make a tags column to recommend the movies from


*  overview
*  genre-extract the specific word-(action ,horror)
* keywords-only the specific keywords
* cast- main three actors
* crew- only director



"""

#missing data
movies.isnull().sum()

"""3 overview rows are missing - so drop them"""

movies.dropna(inplace=True)

movies.duplicated().sum()

movies.iloc[0]

movies.iloc[0].genres

import ast
def convert(obj):
  L=[]
  for i in ast.literal_eval(obj):
    L.append(i['name'])
  return L

movies['genres']=movies['genres'].apply(convert)

movies['keywords']=movies['keywords'].apply(convert)

movies['cast'][0]

"""take first three character name only"""

import ast
def convert_cast(obj):
  L=[]
  counter=0
  for i in ast.literal_eval(obj):
    if counter!=3:
      L.append(i['name'])
      counter+=1
    else:
      break
  return L

movies['cast']=movies['cast'].apply(convert_cast)

movies.head()

movies['crew'][0]

"""lets only get the the director name"""

import ast
def fetch_director(obj):
  L=[]
  for i in ast.literal_eval(obj):
    if i['job']=='Director':
      L.append(i['name'])
  return L

movies['crew']=movies['crew'].apply(fetch_director)

movies.head()

"""lets conert the overview column to list inorder to concat that into a list with cast,crew,genres,keywords to make it as a single tag column"""

movies['overview']=movies['overview'].apply(lambda x:x.split())

movies.head()

"""Sam Worthington - lets remove the space between th words to avoid unnecessary confusions

"""

movies['genres']=movies['genres'].apply(lambda x:[i.replace(" ","")for i in x])
movies['keywords']=movies['keywords'].apply(lambda x:[i.replace(" ","")for i in x])
movies['cast']=movies['cast'].apply(lambda x:[i.replace(" ","")for i in x])
movies['crew']=movies['crew'].apply(lambda x:[i.replace(" ","")for i in x])

movies.head()

movies['tags']=movies['overview']+movies['genres']+movies['cast']+movies['crew']

new_df=movies[['movie_id','title','tags']]
new_df

"""now lets covert the tags---list into string"""

new_df['tags']=new_df['tags'].apply(lambda x:' '.join(x).lower())

new_df.head()

new_df['tags'][3]

new_df['tags'][17]

"""here there is a problem in this features name---
1. lot of similar words are getting repeated- eg : activity , activites
2. numbers

To remove this -we use stemming ,a technique that reduces words to their base or root form, also known as the stem
"""

from nltk.stem.porter import PorterStemmer
ps=PorterStemmer()

def stem(text):
  y=[]
  for i in text.split():
    y.append(ps.stem(i))
  return ' '.join(y)

new_df['tags']=new_df['tags'].apply(stem)

new_df.head()

"""now we have to find the similarity between the tags of each movie Using vectorization

we will use bag of words
* Build a vocabulary from all the unique words in the dataset.

* Represent each document (text) as a vector where:

       i  Each element corresponds to a word in the vocabulary.
       ii  The value is typically the count (frequency) of that word in the document.

Doc1: "I love NLP"

Doc2: "I love deep learning"

Step 1: Create a Vocabulary
["I", "love", "NLP", "deep", "learning"]

Step 2: Vector Representation

Doc1 → [1, 1, 1, 0, 0]

Doc2 → [1, 1, 0, 1, 1]

Each position in the vector corresponds to a word in the vocabulary. Values are word counts.


here it takes most common(5000) words from all the tags.

remove the stop words
"""

from sklearn.feature_extraction.text import CountVectorizer
cv=CountVectorizer(max_features=5000,stop_words='english')

vectors=cv.fit_transform(new_df['tags']).toarray()

vectors.shape

len(cv.get_feature_names_out())

cv.get_feature_names_out()

"""lets calculate the cosine similarity between the vectors"""

from sklearn.metrics.pairwise import cosine_similarity

similarity=cosine_similarity(vectors)

cosine_similarity(vectors).shape

similarity[0]

sorted(similarity[0],reverse=True)

"""In that case it may loose the index position --- so use enumerate to preserve the index position with its similarity value"""

#it returns the movie index
new_df[new_df['title']=='Spectre'].index[0]

sorted(list(enumerate(similarity[0])),reverse=True,key=lambda x:x[1])[1:6]

"""so it returns the similarity score with the index

lets sort the array and get the first five highly similar movies id as output
"""

def recommend(movie):
  movie_index=new_df[new_df['title']==movie].index[0]
  distances=similarity[movie_index]
  movie_list=sorted(list(enumerate(distances)),reverse=True,key=lambda x:x[1])[1:6]

  for i in movie_list:
    print(new_df.iloc[i[0]].title)# returns the movie title of the corresponding indecies

recommend('Spectre')

import pickle

pickle.dump(new_df,open('movies.pkl','wb'))

pickle.dump(new_df.to_dict(),open('movies_dict.pkl','wb'))

pickle.dump(similarity,open('similarity.pkl','wb'))